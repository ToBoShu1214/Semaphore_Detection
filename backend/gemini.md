# 旗語動作偵測系統 v2.0 (Web App)

## 📘 專案簡介
本專案是一個基於電腦視覺的**全端旗語辨識 Web 應用程式**。它使用 **FastAPI (Python)** 作為後端，**React (TypeScript)** 作為前端，讓使用者能透過攝影機，即時進行旗語的練習與辨識。

系統的核心是 **YOLOv8-Pose** 模型，用於偵測人體姿態，並結合 **ONNX** 格式的旗幟偵測模型，實現了在多人場景下自動鎖定旗手的功能。為了提升使用者體驗，專案內建了完整的手勢啟動/重置系統、多種練習與考試模式，以及一套嚴謹的狀態機來確保辨識的穩定與準確。

## ⚙️ 主要元件說明
| 元件 | 功能描述 |
| :--- | :--- |
| `main.py` | **後端伺服器 (FastAPI)**：建立 Web 伺服器，並透過 WebSocket 提供即時通訊端點 (`/ws`)，負責接收前端指令與傳送偵測結果。 |
| `yolo_logic.py` | **核心偵測引擎 (Python)**：封裝了所有電腦視覺處理和狀態機邏輯。其核心是**雙模型協同機制**：<br>    - **姿態追蹤模型 (`yolov8n-pose.pt`)**: 在每一幀持續追蹤畫面中所有人的身體姿態和ID。<br>    - **旗幟偵測模型 (`flag.onnx`)**: 在需要尋找目標時（例如程式啟動時），偵測畫面中的旗幟。<br>    - **IoU 鎖定邏輯**: 透過計算「手腕」與「旗幟」的重疊率 (IoU)，系統能智慧地判斷並鎖定正在操作旗幟的使用者為目標，然後交由姿態追蹤模型進行持續偵測。 |
| `VideoStream.tsx` | **前端介面 (React)**：建構了使用者看到的所有 UI 介面，包含視訊畫面渲染、模式切換、資料視覺化，並透過 WebSocket 與後端進行互動。 |

## 🤖 核心邏輯：狀態機
為了確保辨識的穩定性，系統圍繞一個核心狀態機運作，主要狀態如下：

1.  **`IDLE` (閒置狀態):**
    *   系統的初始狀態。在鎖定目標後，會提示使用者做出「開始手勢」。
    *   **→** 偵測到完整「開始手勢」後，進入 `WAITING` 狀態。

2.  **`WAITING` (等待就位):**
    *   提示使用者做出「雙手垂直放下」的準備姿勢。
    *   **→** 偵測到準備姿勢後，進入 `READY` 狀態。

3.  **`READY` (準備就緒):**
    *   系統待命中，提示使用者可以開始比劃旗語數字。
    *   **→** 偵測到任何一個數字姿勢後，進入 `DETECTING` 狀態。

4.  **`DETECTING` (穩定偵測中):**
    *   確認使用者是否穩定維持同一個姿勢。若穩定超過 `STABLE_DELAY` (0.4秒)，則進入 `GRACE_PERIOD`。
    *   **→** 若姿勢變換，則退回 `READY` 狀態。

5.  **`GRACE_PERIOD` (手臂伸直緩衝期):**
    *   此階段會驗證手臂是否伸直。
    *   **→** 若手臂伸直，數字有效，進入 `COOLDOWN` 狀態。
    *   **→** 若手臂未伸直，提示使用者並退回 `READY` 狀態。

6.  **`COOLDOWN` (冷卻中):**
    *   一個數字成功記錄後進入此狀態，強制使用者必須再次「雙手放下」才能輸入下一個數字，避免重複輸入。
    *   **→** 偵測到準備姿勢後，回到 `READY` 狀態。

---

## 🚀 專案架構更新：全端 Web 應用程式 (2025-11-14)

### 一、架構演進
專案已從單一的 Python 命令列腳本，重構為一個功能完整的**全端 Web 應用程式**。此架構採用前後端分離模式，提供了更豐富的互動體驗與更強大的功能。

- **後端 (Backend):** 使用 **Python FastAPI** 框架，負責處理核心的電腦視覺邏輯與即時通訊。
- **前端 (Frontend):** 使用 **React (TypeScript)** 框架，負責使用者介面、互動控制與資料視覺化。

### 二、新版檔案結構
```
📁 Semaphore-Detection-Web
│
├── 📁 backend
│   ├── main.py               # FastAPI 後端主程式，處理 WebSocket 通訊
│   ├── yolo_logic.py         # 核心偵測邏輯 (由舊版 yolo_pose_angle.py 重構而來)
│   ├── requirements.txt      # Python 依賴項
│   ├── yolov8n-pose.pt       # 預設的人體姿勢模型
│   ├── flag.onnx             # 旗幟偵測模型 (ONNX 格式)
│   └── mapping.csv           # 數字序列與文字的對應表
│
└── 📁 frontend
    ├── 📁 public
    │   └── 📁 digits          # 存放 0-9 的數字圖示 (0.png, 1.png, ...)
    └── 📁 src
        ├── App.tsx             # React 應用程式根組件
        ├── VideoStream.tsx     # 核心前端組件，包含所有 UI 與互動邏輯
        └── index.tsx           # React 應用程式進入點
```

### 三、主要功能列表 (Features)
- **雙模式系統**:
    - **自由練習模式**: 使用者可自由比劃旗語，系統會即時辨識並將四位數字轉譯為對應文字。
    - **考試模式**: 根據預設題目進行測驗，系統會判斷動作是否正確並計時。
- **目標自動鎖定**: 在多人場景下，系統能自動偵測旗幟並鎖定旗手，確保偵測目標的準確性。
- **即時影像分析**: 透過 WebSocket 即時串流攝影機畫面，並在前端顯示偵測結果。
- **手勢啟動/重置**: 使用者可透過特定的揮手交叉手勢，來啟動或重置辨識流程，無需操作鍵盤滑鼠。
- **取消 (退格) 手勢**:
    - **手勢**: 左手 45 度，右手 135 度。
    - **功能**: 在練習模式中，此手勢可刪除最後一個輸入的數字；若數字序列為空，則會刪除歷史記錄中的最後一個文字，並回到前三個數字的狀態。
- **文字歷史記錄**: 系統會記錄所有成功辨識出的文字，並顯示於歷史記錄區。
- **指定字串練習**:
    - 在練習模式中，可開啟「指定練習」開關。
    - 使用者可輸入任意長度的中文字串 (例如「你好」)。
    - 系統會反向查詢數字序列，並透過圖示和文字，引導使用者逐一完成每個字的旗語動作。

### 四、後端說明 (FastAPI)
後端的核心是 `main.py`，它建立了一個 FastAPI 伺服器，並透過 `/ws` 路徑提供一個 **WebSocket 端點**。

- **即時通訊:** 所有前端與後端的通訊都透過 WebSocket 進行，實現了低延遲的視訊流與資料傳輸。
- **指令驅動:** 後端接收來自前端的 JSON 格式指令，例如：
    - `set_mode`: 切換**練習 (practice)** 或 **考試 (exam)** 模式。
    - `set_challenge_mode`: 啟用/停用指定練習模式，並傳遞目標字串。
    - `start_exam` / `stop_exam`: 控制考試流程。
- **資料流:** 後端執行 `yolo_logic.py` 中的偵測邏輯，並將包含**視訊幀 (base64)** 和**偵測資料 (JSON)** 的 payload 持續傳送給前端。

### 五、前端說明 (React)
前端的主要介面由 `VideoStream.tsx` 組件構成。

- **視訊渲染:** 它接收後端傳來的 base64 圖片，並將其繪製到 HTML `<canvas>` 元素上。
- **互動控制面板:** 提供了一個全面的 UI，讓使用者可以：
    - **模式選擇:** 自由切換「練習模式」與「考試模式」。
    - **指定練習:** 在練習模式中，可開啟「指定練習」開關，輸入目標字串，系統會提供圖文引導。
- **資料視覺化:** 即時顯示後端傳來的偵測狀態，包括：
    - **自由練習**: 當前角度、偵測數字、已輸入序列、最終翻譯結果與提示。
    - **指定練習**: 目標字串、目前進度、下一個數字的圖示提示。
    - **通用**: 文字歷史記錄、系統狀態提示等。
- **狀態管理:** 處理載入中、連線中斷、後端錯誤等各種狀態，並提供明確的 UI 反饋。

### 六、如何執行
本專案支援 CPU 和 GPU 兩種執行模式。預設為 CPU 模式。

1.  **標準執行 (CPU 模式):**
    為了確保環境穩定，建議使用 Conda 建立一個獨立的環境來運行後端。
    *   **環境設定 (僅需執行一次):**
        在您的 Anaconda Prompt 或終端機中，執行以下命令：
        ```bash
        # 1. 建立一個新的 Conda 環境 (例如命名為 'cpu_env')
        conda create -n cpu_env python=3.11 -y

        # 2. 啟用新環境
        conda activate cpu_env

        # 3. 安裝純 CPU 版本的 onnxruntime
        conda install -c conda-forge onnxruntime -y

        # 4. 安裝其餘的 Python 依賴項
        pip install fastapi uvicorn websockets numpy opencv-python ultralytics Pillow python-multipart onnx
        ```
    *   **啟動後端:**
        每次運行後端時，請確保您已啟用 `cpu_env` 環境，然後執行以下指令：
        ```bash
        # 進入後端資料夾
        cd Semaphore-Detection-Web/backend

        # 啟用環境 (如果尚未啟用)
        conda activate cpu_env

        # 啟動伺服器
        uvicorn main:app --host 0.0.0.0 --port 8000 --reload
        ```
    *   **啟動前端:**
        在 `frontend` 目錄下，執行以下指令 (需先執行 `npm install` 或 `yarn install`)。
        ```bash
        # 進入前端資料夾
        cd Semaphore-Detection-Web/frontend

        # 啟動開發伺服器
        npm start
        ```

3.  **訪問應用程式:**
    啟動前後端後，在瀏覽器中開啟 `http://localhost:3000` 即可看到應用程式介面。

### 七、開發進度日誌
| 日期 | 進度 |
| :--- | :--- |
| 2025/11/16 | **GPU 環境深度除錯 (GPU Environment Deep-Debugging)**<br>- **目標**: 解決 `onnxruntime-gpu` 初始化時的 `LoadLibrary failed with error 126` 錯誤。<br>- **已執行步驟**:<br>  1. 確認 CUDA 12.1 和 cuDNN 8.9.7 已正確安裝且檔案位置無誤。<br>  2. 修改 `yolo_logic.py`，在程式啟動時手動將 CUDA `bin` 目錄加入 DLL 搜尋路徑。<br>  3. 清除 `__pycache__` 以確保執行的是最新的程式碼。<br>  4. 確認 `CUDA_PATH` 環境變數設定正確，並排除 Anaconda 環境中存在衝突套件的可能性。<br>  5. 強制重新安裝 `onnxruntime-gpu` 套件。<br>  6. 修改 `yolo_logic.py` 中的錯誤處理邏輯，使其在 GPU 初始化失敗時能成功降級 (fallback) 至 CPU 模式。<br>- **目前狀態**: CPU 降級模式已可正常運作。GPU 模式的 `LoadLibrary` 錯誤依然存在。<br>- **下一步**: 使用者將安裝/修復 **Microsoft Visual C++ 可轉散發套件** (`vc_redist.x64.exe`) 並**重新啟動電腦**，這是解決此問題的最後一個關鍵步驟。重啟後將再次嘗試執行 `python main.py`。 |
| 2025/11/16 | **GPU 加速支援與環境配置 (GPU Acceleration Support & Environment Setup)**<br>- **後端程式碼調整**: 實作了後端 `yolo_logic.py` 的動態 GPU 偵測邏輯，程式會自動判斷並優先使用 `CUDAExecutionProvider`，若無則退回 `CPUExecutionProvider`。<br>- **依賴項管理**: 建立了 `requirements-gpu.txt` 檔案，用於安裝 GPU 版本的 Python 依賴項，並在 `gemini.md` 中提供了詳細的安裝指南。<br>- **日誌修正**: 修正了 ONNX 模型載入時的日誌訊息，確保其能準確反映模型實際運行的提供者 (CPU/GPU)。<br>- **環境問題排查**: 協助使用者排查 CUDA Toolkit 和 cuDNN 的安裝問題，包括版本選擇 (推薦 CUDA 12.1, cuDNN v8.9.7)、安裝時跳過驅動程式、以及解決 `LoadLibrary failed with error 126` 的環境變數問題 (建議重啟電腦)。 |
| 2025/11/16 | **雙模型目標鎖定與系統穩定性重構 (Dual-Model Target Locking & Stability Refactoring)**<br>- **核心功能**: 實作了「雙模型協同」的目標鎖定機制。系統會先使用輕量的 `flag.onnx` 模型偵測旗幟，再透過計算手腕與旗幟的 IoU (交集比)，從多人場景中智慧鎖定旗手，最後由 `yolov8n-pose.pt` 模型對鎖定的目標進行持續的姿態追蹤。<br>- **穩定性強化**: 引入了目標丟失寬限期，解決了因短暫遮擋而中斷流程的問題；並完善了「結束手勢」與目標丟失時的狀態重置邏輯，確保文字和數字序列被完整清除。<br>- **效能優化**: 根據使用者需求，調整了旗幟偵測的觸發時機，在鎖定目標後便停止偵測，以提升幀率。<br>- **顯示錯誤修復**: 徹底解決了前端重複繪製、後端座標未縮放所導致的「雙重/錯位邊界框」問題，確保框體顯示的唯一性與準確性。 |
| 2025/11/14 | **互動功能深度開發 (Interactive Feature Development)**<br>- **手勢偵測優化**: 根據使用者回饋，大幅優化了「揮手啟動」手勢的偵測邏輯，包含放寬手腕交叉距離、區分上下揮動的手臂伸直標準、移除過於嚴苛的計數重置邏輯，使其更靈敏也更符合人體工學。<br>- **新增「取消/退格」手勢**: 實作了「左手45度、右手135度」的取消手勢，可用於刪除最後一個數字，或刪除歷史記錄中的最後一個文字，並回到前三個數字的狀態。<br>- **新增「文字歷史記錄」**: 現在所有成功打出的字都會被記錄並顯示在介面上。<br>- **新增「指定字串練習」模式**: 允許使用者輸入任意長度的字串進行練習。系統會反向查詢數字碼，並透過圖示和文字提示，引導使用者逐一完成每個字的旗語動作。此功能包含對應的前後端修改。 |
| 2025/10/19 | **架構重構與功能健檢 (Major Refactoring & Hardening)**：<br>- **核心邏輯重構**：將原有的旗標判斷 (`ready_flag`) 升級為更清晰、更易於擴充的**狀態機**架構。<br>- **命令列參數化**：使用 `argparse` 將寫死的檔案路徑（影片、模型、字體等）全部改為可配置的命令列參數，並支援攝影機輸入。<br>- **Bug 修正**：修復了 `angle_diff` 角度環繞判斷、`np.zeros` 尺寸寫死、偵錯面板顯示不一致等多個 Bug。<br>- **體驗優化**：根據使用者回饋，修正了 `COOLDOWN` 狀態的提示邏輯。<br>- **文件同步**：根據最新程式碼，完整更新此份說明文件。 |
| 2025/10/17 | **穩定性重大更新**：<br>- 導入角度平滑化邏輯。<br>- 新增手臂伸直偵測與遮擋判斷。<br>- 新增「伸直緩衝期」與「智慧辨識」邏輯。<br>- 為主要參數加上註解，並完成文件。 |
| 2025/10/05 | 加入影片輸入功能（`旗語測驗 題目 1.mp4`），並優化紅字提示顯示方式。 |
| 2025/10/02 | 實作佇列機制，能記錄多個穩定姿勢（對應旗語動作序列）。 |
| 2025/09/29 | 導入 `is_stable_pose()`，建立穩定判定規則。 |
| 2025/09/25 | 加入角度計算函式，能區分左右手與旗桿方向。 |
| 2025/09/20 | 使用 YOLOv8n-pose 模型，偵測人體關鍵點。 |

### 八、已知問題
(目前無已知的重大問題。)

### 九、未來規劃
- **硬體加速 (GPU)**：研究並實作模型推論的 GPU 加速。
- **優化人的偵測**：解決即使人一直在畫面中，仍會出現「倒數兩秒請回到畫面」的提示並重新尋找旗手。
